// INF 141 / CS 121
// Kelvin Wu
// Xueyi Fan
package ir.assignments.three;

import java.io.BufferedReader;
import java.util.Collections;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.util.ArrayList;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.StringTokenizer;

/**
 * A collection of utility methods for text processing.
 */
public class Utilities {
	public static int uniqueSubdomain;
	public static int uniquePage;
	public static int longestPageWords;
	public static ArrayList<Frequency> commonWords;
	public BufferedReader br;
	public FileReader fr;
	public static FileWriter fw;
	public static BufferedWriter bw;
	public File stopwords;
	public File data;
	public static int docIdCount;
	// the list to load stopwords that we should remove when counting common
	// words
	public static Set<String> stopwordSet;
	public static Map<String, Integer> frequencyMap;
	public static Map<String, Set<String>> subdomainUnique;
	public static Map<Integer, ArrayList<String>> reverseMap;
	// using map to store urls and related texts
	// with url as the key and text as value
	// since the key set of a map ensures uniqueness, it is easy to count unique
	// pages after
	public static Map<String, String> entries;
	// similar as above, use map to store pages that duplicates,
	// with url as the key, time of appearances as value
	public static Map<String, Integer> duplicatePages;
	// the set to store unique domains, so that we can count the number easily
	public static Set<String> subdomainSet;

	public static StringTokenizer st;

	public static Map<String, Integer> term2termid;
	public static Map<Integer, ArrayList<Integer>> docid2termlist;
	public static Map<Integer, String> termid2term;
	
	//TODO: implement these indices.
	// Xueyi
	// the inverted index for all terms
	// logic for making the inverted index:
			// if the url is already visited
			//		do not update docId and skip.
	public static Map<Integer, ArrayList<Integer>> termid2docids;
	
	// Kelvin
	public static Map<Integer, Map<Integer, Integer>> docid2termfrequencymap; 
	
	// comparator to sort the words with the same frequencies
	// this will sort the words in alphabetical order
	private static Comparator<String> ALPHABETICAL_ORDER = new Comparator<String>() {
		public int compare(String str1, String str2) {
			int res = String.CASE_INSENSITIVE_ORDER.compare(str1, str2);
			if (res == 0) {
				res = str1.compareTo(str2);
			}
			return res;
		}
	};

	// initialize the variables
	public Utilities(String stopwordsFileName, String dataFileName)
			throws IOException {
		stopwords = new File(stopwordsFileName);
		data = new File(dataFileName);
		stopwordSet = new HashSet<String>();
		frequencyMap = new HashMap<String, Integer>();
		reverseMap = new HashMap<Integer, ArrayList<String>>();
		commonWords = new ArrayList<Frequency>();
		entries = new HashMap<String, String>();
		duplicatePages = new HashMap<String, Integer>();
		subdomainSet = new HashSet<String>();
		subdomainUnique = new HashMap<String, Set<String>>();
		term2termid = new HashMap<String, Integer>();
		docid2termlist = new HashMap<Integer, ArrayList<Integer>>();
		termid2term = new HashMap<Integer, String>();
		docid2termfrequencymap = new HashMap<Integer, Map<Integer,Integer>>();
		termid2docids = new HashMap<Integer, ArrayList<Integer>>();
		//docIdCount = 0; // update when loading entries.
	}

	public void loadData() throws IOException {
		loadStopWords();
		loadEntries();
	}

	// I/O variables are initialized and closed
	// in each method that calls them
	// because thehy are used on different files
	public void loadEntries() throws IOException {
		int index = 0; // for debug
		long startTime = System.currentTimeMillis();

		try {
			fr = new FileReader(data);
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		}
		br = new BufferedReader(fr);

		docIdCount = 0; // assign docID to each page we visit
		for (String line = br.readLine(); line != null; line = br.readLine()) {
			// it is safe to call realine() in the loop because the data is
			// formatted.
			// first line of each entry is its subdomain
			String subdomain = line;
			if (!subdomain.equalsIgnoreCase("ics")) { // we consider ics.uci.edu
														// as the domain. so
														// exclude ics from
														// subdomains
				// a set will be perfect for counting subdomains.
				// remove the '.ics' from subdomain
				subdomain = subdomain.substring(0, subdomain.length() - 4);
				subdomainSet.add(subdomain);
			}
			// System.out.println(subdomainSet); for debug

			// second line of each entry is its url
			line = br.readLine();
			String url = line;

			if (subdomainUnique.containsKey(subdomain))
				subdomainUnique.get(subdomain).add(url);
			else {
				Set<String> urls = new HashSet<String>();
				urls.add(url);
				subdomainUnique.put(subdomain, urls);
			}

			// starting from the third line, until it reaches "ENDOFPAGE" is the
			// text body
			String text = "";
			line = br.readLine();

			ArrayList<Integer> termList = new ArrayList<Integer>(); // stores the term ids of this doc.
			// each page content is marked with "ENDOFPAGE" and page index at
			// the last line.
			
			HashMap<Integer, Integer> termFrequencyMap = new HashMap<Integer, Integer>();
			// update the term frequency of a page
			while (!line.startsWith("ENDOFPAGE")) {
				if (!entries.containsKey(url)) {

					text += line + "\n";

					String[] allTokens = line.split("[^a-zA-Z]+");
					for (int i = 0; i < allTokens.length; i++) {
						int termId = -1;
						String term = allTokens[i];
						term = term.trim();
						if (!term.equals("") && !stopwordSet.contains(term)) {
							if (term2termid.containsKey(term)) {
								termId = term2termid.get(term);
							} else {
								termId = term2termid.keySet().size();
								term2termid.put(term, termId);
								termid2term.put(termId, term);
							}
							termList.add(termId);
							if (termFrequencyMap.containsKey(termId)) {
								termFrequencyMap.put(termId, termFrequencyMap.get(termId) + 1);
							} else {
								termFrequencyMap.put(termId, 1);							
							}
						}
					}
				} else {
					// for later use to increase word frequency appeared in
					// duplicate pages
					if (duplicatePages.containsKey(url)) {
						// if already in the map, increase times of appearances
						duplicatePages.put(url, duplicatePages.get(url) + 1);
					}
					// the times of appearance starts of 2 in duplicate pages
					// since that the system already detect it in the main map
					duplicatePages.put(url, 2);
				}
				line = br.readLine();
			}
			if (!entries.containsKey(url)) { // if doc is not visited before
				// add to the entry list
				entries.put(url, text);
				// update the docIdCount
				docIdCount = docid2termlist.keySet().size();
				docid2termlist.put(docIdCount, termList);
				docid2termfrequencymap.put(docIdCount, termFrequencyMap);
				// update the inverted index for terms in this doc.
				for(int t = 0; t < termList.size(); t++){
					int termId = termList.get(t);
					if(termid2docids.get(termId) == null)
					termid2docids.put(termList.get(t), );
				}
			}
			index++; // for debug
			System.out.println("currently at page " + index); // for debug
		}

		fr.close();

		br.close();

		long endTime = System.currentTimeMillis();

		long duration = endTime - startTime;
		double seconds = (double) duration / 1000;

		System.out.printf("Indexing took : %2.2f seconds \n", seconds);
	}	

	// I/O variables are initialized and closed
	// in each method that calls them
	// because thehy are used on different files
	// load the file that contains stopwords to remove
	public void loadStopWords() throws IOException {
		try {
			fr = new FileReader(stopwords);
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		}
		br = new BufferedReader(fr);
		for (String line = br.readLine(); line != null; line = br.readLine()) {
			stopwordSet.add(line);
		}
		fr.close();
		br.close();
	}

	// I/O variables are initialized and closed
	// in each method that calls them
	// because thehy are used on different files
	// simply takes the subdomain set we created when loading entries and print
	public static void writeToSubdomains(String outputFilename)
			throws IOException {
		fw = new FileWriter(new File(outputFilename));
		bw = new BufferedWriter(fw);

		for (String s : subdomainUnique.keySet())
			bw.write(s + " | number of unique pages: "
					+ subdomainUnique.get(s).size() + "\n");

		// fw.close();
		bw.close();
	}

	// I/O variables are initialized and closed
	// in each method that calls them
	// because thehy are used on different files
	// create frequency objects for words that are read from the content
	// associated
	// with each url
	// words from urls that are not unique have their frequencies multiplied
	// by the time that url appears
	// also checks if the word is in the stopword set. if it is stop word,
	// simply dont add to the frequency list.
	public static void writeToCommonWords(String outputFileName)
			throws IOException {

		// extract the words from each page.
		for (String s : entries.keySet()) {
			st = new StringTokenizer(entries.get(s));
			String token;
			while (st.hasMoreTokens()) {
				token = st.nextToken();
				if (!stopwordSet.contains(token)) {// if is stopword, skip
					if (duplicatePages.containsKey(s)) { // if parent url is
															// duplicated
						if (frequencyMap.containsKey(token)) { // if already
																// appeared
							frequencyMap.put(token, frequencyMap.get(token)
									+ duplicatePages.get(s));
						}
						frequencyMap.put(token, duplicatePages.get(s));
					}
					// if not appeared in a duplicate page
					else if (frequencyMap.containsKey(token)) { // if already
																// appeared
						frequencyMap.put(token, frequencyMap.get(token) + 1);
					} else {
						// if not in duplicate pages nor already in set
						frequencyMap.put(token, 1);
					}
				}
			}
		}

		// so not the frequency map stores the words and its times of
		// appearances.
		// now we use another map to store the reverse: frequency value as key,
		// a list of words have that frequency as value
		// the reason lies in that so that we can sort the keys of reverseMap so
		// that top frequency are sorted
		// then sort the list associated with the frequencies, so that words are
		// sorted in alphabetical order.

		for (String s : frequencyMap.keySet()) {
			int frequency = frequencyMap.get(s);
			if (reverseMap.containsKey(frequency)) {
				reverseMap.get(frequency).add(s);
			} else {
				ArrayList<String> newList = new ArrayList<String>();
				newList.add(s);
				reverseMap.put(frequency, newList);
			}
		}

		// now we add the reverseMap keys(frequencies) to an intermediate list
		// and sort it in descending order
		ArrayList<Integer> frequencies = new ArrayList<Integer>();
		for (Integer i : reverseMap.keySet()) {
			frequencies.add(i);
		}

		Collections.sort(frequencies, Collections.reverseOrder());
		// now we sort the words in alphabetical order
		// and create frequencies for them
		// then add to the commonword list
		ArrayList<String> temp;
		for (int i = 0; i < 500; i++) {
			temp = new ArrayList<String>();
			temp.addAll(reverseMap.get(frequencies.get(i)));

			Collections.sort(temp, ALPHABETICAL_ORDER);

			for (int j = 0; j < reverseMap.get(frequencies.get(i)).size(); j++) {
				Frequency fr = new Frequency(temp.get(j), frequencies.get(i));
				commonWords.add(fr);
			}
		}

		// now we write the top words and thei frequencies to the file.

		fw = new FileWriter(new File(outputFileName));
		bw = new BufferedWriter(fw);
		for (int i = 0; i < 500; i++) {
			bw.write("rank :" + (i + 1) + " " + commonWords.get(i).getText()
					+ " | " + commonWords.get(i).getFrequency() + "\n");
		}
		bw.close();

	}

	public static Integer numberOfDocuments() {
		return docid2termlist.keySet().size();
	}

	public static Integer numberOfUniqueWords() {
		return termid2term.keySet().size();
	}

	public static void writeIndexToFile() {
		try {
			File file = new File("term2termid");
			FileOutputStream f;
			ObjectOutputStream s;
			f = new FileOutputStream(file);
			s = new ObjectOutputStream(f);
			s.writeObject(term2termid);
			s.close();

			file = new File("termid2term");
			f = new FileOutputStream(file);
			s = new ObjectOutputStream(f);
			s.writeObject(termid2term);
			s.close();

			file = new File("docid2termlist");
			f = new FileOutputStream(file);
			s = new ObjectOutputStream(f);
			s.writeObject(docid2termlist);
			s.close();

			// FileInputStream r = new FileInputStream("term2termid");
			// ObjectInputStream t = new ObjectInputStream(r);
			// HashMap<String, Integer> fileObj2 = (HashMap<String, Integer>)
			// t.readObject();
			// t.close();
			// System.out.println(term2termid.hashCode());
			// System.out.println(fileObj2.hashCode());

		} catch (FileNotFoundException e1) {
			// TODO Auto-generated catch block
			e1.printStackTrace();
		} catch (IOException e1) {
			e1.printStackTrace();
		}
		// } catch (ClassNotFoundException e1) {
		// e1.printStackTrace();
		// }

	}

	// since the text we passed to this class is parsed text without the
	// html markups, we dont need to worry about them.
	public static String longestPage() {
		String url = "";
		longestPageWords = 0;
		for (String s : entries.keySet()) {
			String text = entries.get(s);
			String words = text.trim();
			int count = words.split("\\W+").length;
			if (count >= longestPageWords) {
				longestPageWords = count;
				url = s;
			}
		}
		return url;
	}
	
	public static void calculateTFIDF() {
		
	}

	public static void main(String[] args) throws IOException {
		Utilities utl = new Utilities("stopwords.txt", "data.txt");
		utl.loadData();
		writeToSubdomains("Subdomains.txt");
		writeToCommonWords("CommonWords.txt");
		System.out.println("number of unique pages: " + entries.size());
		System.out.println("number of subdomains: " + subdomainSet.size());
		System.out.println("longest page and word number: " + longestPage()
				+ " | " + longestPageWords + " words");
		System.out.println("Number of documents: " + numberOfDocuments());
		System.out.println("Unique Words: " + numberOfUniqueWords());
		writeIndexToFile();
	}
}
